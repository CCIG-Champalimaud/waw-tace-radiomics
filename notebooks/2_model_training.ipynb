{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141b9f36-d761-4988-83f2-3f06bf320109",
   "metadata": {},
   "source": [
    "# Model training and tuning\n",
    "\n",
    "Training and tuning a model for the best hyperparameters can be complicated. There are many choices when it comes to models, preprocessing strategies and even data-specific strategies.\n",
    "\n",
    "## Data-specific strategies\n",
    "\n",
    "Let's carefully consider the data we have: we know that some patients have multiple lesions, but we only have outcome data on a per patient level - there is reasonable way of knowing the outcome on a per lesion level!\n",
    "\n",
    "So we are faced with a dilema: \n",
    "\n",
    "1. If we use all lesions, the same patient target has to be valid for multiple lesions, but does this make sense? We are ultimately implying that all lesions contribute equally to the outcome of the patient when we know this not to be the case!\n",
    "2. If we use only the largest lesion we may be losing information present in smaller lesions!\n",
    "\n",
    "Here, we chose a compromise: we will be working with features from the largest lesion and including lesion sizes until the third largest lesion. So, if an individual has no other lesion, then $\\mathrm{2nd\\ largest\\ lesion\\ size} = 0$ and $\\mathrm{3rd\\ largest\\ lesion\\ size} = 0$. Otherwise, these values will be positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291e417-24fc-4a5f-970c-c709011a56d7",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Data pre-processing typically involves strategies dedicated to:\n",
    "\n",
    "- **Normalising** or **standardising data**: each individual feature is transformed, making all features follow a distribution with common parameters (typically $\\mu = 0$ and $\\sigma=1$). This makes comparisons between features considerably easier and is necessary for some models to adequately converge\n",
    "- **Removing correlated features**: removing correlated features is also of paramount importance, particularly when considering radiomic feature models. In this scenario, we are often faced with datasets where the number of samples is relatively reduced, especially when compared with the number of features. Removing correlated features reduces the possibility of spurious associations \n",
    "- **Removing features which are not associated with the target variable**: a lot of features may be completely unrelated to the target variable, so assessing their association with the target variable is an easy way of performing this. For instance, if a variable shows a statistically significant difference between different classes, it is likely that this is a good predictor! This may sound familiar to some of you who may have performed survival analyses - typically, in these analyses, an initial set of single variable analysis (i.e. log-rank test) are performed to select statistically significant variables which are then used for multiple variable Cox proportional hazards model to control for feature interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624b6f8-c65e-48e5-99d9-190583c3050a",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "We should take note of some key aspects of **model selection** when working with tabular data:\n",
    "\n",
    "* If **performance** is key, maybe go for tree-based models ([random forests](https://www.datacamp.com/tutorial/random-forests-classifier-python), [XGBoost](https://www.kdnuggets.com/2020/12/xgboost-what-when.html) or [LightGBM](https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/)). However, keep in mind the following - with small datasets, there is only so much we can do! While tree-based models do what they can to reduce overfitting, the fact is: when we have a relatively small amount of data, overfitting may be right around the corner!\n",
    "* If **interpretability** is key, maybe for linear models. A good example of this is [elastic net regression](https://analyticsindiamag.com/ai-mysteries/hands-on-tutorial-on-elasticnet-regression/#h-what-is-elastic-net). This regression method tries to simultaneously shrink and eliminate features which are highly correlated. However, not everything is perfect - due to their simplicity, these models may fail to capture complex relationships within the data. As such, their performance is possibly quite reduced\n",
    "\n",
    "## What is to be done?\n",
    "\n",
    "1. Use random forest with nested CV to obtain the expected hyperparameter tuning performance\n",
    "2. Use CV to select best hyperparameters\n",
    "3. Retrain the model using the entire train dataset with the best parameters\n",
    "4. Compare models with models trained on simpler, widely available data (i.e. laboratory results and lesion diameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a180c5-1fca-4e99-88f8-a9b78000d583",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We will use here the complete set of features from WAW-TACE as extracted in the previous section. We start by selecting the largest lesion and by filling in the lesion sizes for the 2nd and 3rd largest lesions (if available). For the rest of the data we will be using **disease progression** as our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e965534-f2c7-452c-b8b7-0f827d8d5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# relevant paths and constants are defined here\n",
    "clinical_data_target_path = \"/big_disk/Datasets/WAW-TACE/clinical_data_wawtace_v2_15_07_2024.xlsx\"\n",
    "arterial_feature_df_path = \"../data/all_data_1.csv\"\n",
    "portal_feature_df_path = \"../data/all_data_2.csv\"\n",
    "size_feature = \"original_shape_VoxelVolume\"\n",
    "\n",
    "df_target = pd.read_excel(clinical_data_target_path)\n",
    "dfs = {\n",
    "    \"Arterial\": pd.read_csv(arterial_feature_df_path),\n",
    "    \"Portal\": pd.read_csv(portal_feature_df_path),\n",
    "}\n",
    "\n",
    "new_df = {\n",
    "    \"Arterial\": [], \n",
    "    \"Portal\": []\n",
    "}\n",
    "for k in dfs:\n",
    "    df = dfs[k]\n",
    "    n = 0\n",
    "    for patient in df[\"identifier\"].unique():\n",
    "        sub_df = df[df[\"identifier\"] == patient].copy()\n",
    "        if sub_df.shape[0] > 1:\n",
    "            keep_lesion = sub_df.sort_values(size_feature).iloc[-1] # keep only last (largest)\n",
    "            lesion_sizes = sorted(sub_df[size_feature])[:-1][::-1] # exclude last (largest)\n",
    "            if len(lesion_sizes) == 1:\n",
    "                lesion_sizes = [lesion_sizes[0], 0]\n",
    "            keep_lesion[\"2nd_largest_size\"] = lesion_sizes[0]\n",
    "            keep_lesion[\"3nd_largest_size\"] = lesion_sizes[1]\n",
    "            sub_df = pd.DataFrame(keep_lesion).T\n",
    "        else:\n",
    "            sub_df.loc[:, \"2nd_largest_size\"] = 0\n",
    "            sub_df.loc[:, \"3nd_largest_size\"] = 0\n",
    "        cols_to_drop = [\"phase\", \"mask_idx\"]\n",
    "        cols_to_drop.extend([x for x in sub_df.columns if \"diagnostics\" in x])\n",
    "        cols_to_drop.extend([x for x in sub_df.columns if \"wavelet\" in x])\n",
    "        sub_df.drop(cols_to_drop, inplace=True, axis=1)\n",
    "        sub_df.columns = [f\"{x}_{k}\" if x != \"identifier\" else x for x in sub_df.columns]\n",
    "        new_df[k].append(sub_df)\n",
    "        n += 1\n",
    "\n",
    "new_df[\"Arterial\"] = pd.concat(new_df[\"Arterial\"], axis=0)\n",
    "new_df[\"Portal\"] = pd.concat(new_df[\"Portal\"], axis=0)\n",
    "\n",
    "radiomics_df = pd.merge(new_df[\"Arterial\"], new_df[\"Portal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8c245-2ccc-450e-92e8-7fe2e60e2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = \"progression\"\n",
    "\n",
    "relevant_columns = [\n",
    "    \"PATPRI\", # patient identifier\n",
    "    \"age\", # patient age\n",
    "    \"gender_woman\", # whether patient identifies as woman\n",
    "    \"etiology_mixed\", # whether tumor etiology is mixed\n",
    "    \"etiology_HCV\", # whether tumor etiology is hep C\n",
    "    \"etiology_HBV\", # whether tumor etiology is hep B\n",
    "    \"etiology_alcoholic\", # whether tumor etiology is alcoholic\n",
    "    \"etiology_NASH\", # whether tumor etiology is nonalcoholic steatohepatitis\n",
    "    \"etiology_cryptogenic\", # whether etiology is unknown\n",
    "    \"lab_albumin\", # albumin concentration\n",
    "    \"lab_creatinine\", # creatinine concentration\n",
    "    \"lab_bilirubin\", # bilirubin concentration\n",
    "    \"lab_afp\", # alfa-fetoprotein concentration\n",
    "    \"lab_inr\", # prothrombin international normalised ratio \n",
    "    \"lab_alt\", # alanine aminotransferase\n",
    "    \"lesion1_diameter\", # largest lesion diameter\n",
    "    \"lesion2_diameter\", # 2nd largest lesion diameter\n",
    "    \"lesion3_diameter\", # 3rd largest lesion diameter\n",
    "    target_var,\n",
    "]\n",
    "\n",
    "df_target_processed = df_target[relevant_columns].copy()\n",
    "df_target_processed[\"identifier\"] = df_target_processed[\"PATPRI\"]\n",
    "df_target_processed[\"class\"] = df_target_processed[target_var]\n",
    "df_target_processed = df_target_processed.drop([\"PATPRI\", target_var],axis=1)\n",
    "df_class = df_target_processed.loc[:, [\"identifier\", \"class\"]]\n",
    "df_clinical = df_target_processed.drop([\"class\"],axis=1)\n",
    "df_clinical[\"lesion2_diameter\"] = df_clinical[\"lesion2_diameter\"].fillna(0)\n",
    "df_clinical[\"lesion3_diameter\"] = df_clinical[\"lesion3_diameter\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f830db-8d5e-4140-a02a-b30103083245",
   "metadata": {},
   "source": [
    "## Create data splits\n",
    "\n",
    "Here, we are setting aside 20% of the data for a hold-out test set. We will not touch the hold-out test data except for the actual testing at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2ad44-78a9-41da-8071-3c05ad99bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_patients\") as o:\n",
    "    test_patients = [x.strip() for x in o.readlines()]\n",
    "\n",
    "train_class_df = df_class.iloc[~np.in1d(df_class.identifier, test_patients)]\n",
    "\n",
    "train_radiomics_df = radiomics_df.iloc[~np.in1d(radiomics_df.identifier, test_patients)]\n",
    "train_clinical_df = df_clinical.iloc[~np.in1d(df_clinical.identifier, test_patients)]\n",
    "\n",
    "train_radiomics_df = pd.merge(train_radiomics_df, train_class_df)\n",
    "train_clinical_df = pd.merge(train_clinical_df, train_class_df)\n",
    "train_full_df = pd.merge(\n",
    "    pd.merge(train_radiomics_df, train_class_df),\n",
    "    train_clinical_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4cd1a-baf4-4682-97ef-c4b83c3b95a5",
   "metadata": {},
   "source": [
    "## Inspecting the data - minimal exploratory data analysis\n",
    "\n",
    "Let us now take the time to plot the data and some of the features and see how they are distributed. Here, we will pick the features showing the highest association with our target variable according to a t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d943507-2d96-4701-90e1-c5aa143b082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats # library to perform statistics in Python\n",
    "\n",
    "ttest_results = {}\n",
    "target = train_radiomics_df[\"class\"]\n",
    "for x in train_radiomics_df:\n",
    "    if x not in [\"identifier\", \"phase\", \"mask_idx\", \"class\"]:\n",
    "        feature = train_radiomics_df[x]\n",
    "        ttest_result = stats.ttest_ind(\n",
    "            feature[target == 0].astype(float), \n",
    "            feature[target == 1].astype(float))\n",
    "        if ttest_result.pvalue < 0.05:\n",
    "            ttest_results[x] = ttest_result.statistic\n",
    "\n",
    "ttest_results_clinical = {}\n",
    "target = train_clinical_df[\"class\"]\n",
    "for x in train_clinical_df:\n",
    "    if x not in [\"identifier\", \"class\"] and \"etiology\" not in x:\n",
    "        feature = train_clinical_df[x]\n",
    "        ttest_result = stats.ttest_ind(\n",
    "            feature[target == 0].astype(float), \n",
    "            feature[target == 1].astype(float))\n",
    "        if ttest_result.pvalue < 0.05:\n",
    "            ttest_results_clinical[x] = ttest_result.statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272248a7-46d8-4236-a499-2e0600812537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sbn # plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_n = 5\n",
    "\n",
    "ttest_results_sorted = sorted(ttest_results.keys(), key=lambda k: -ttest_results[k] ** 2)\n",
    "\n",
    "for key in ttest_results_sorted[:top_n]:\n",
    "    sbn.boxplot(x=\"class\", y=key, data=train_radiomics_df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a68ec6-d889-4046-a6d5-136338968630",
   "metadata": {},
   "source": [
    "We now do the same for the clinical features. Clearly we see there are more radiomics features which are associated with the target variable - this could be a good indication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38eed2c-601f-4af9-969b-3dc33e499669",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_results_sorted = sorted(ttest_results_clinical.keys(), key=lambda k: -ttest_results_clinical[k] ** 2)\n",
    "\n",
    "for key in ttest_results_sorted[:top_n]:\n",
    "    sbn.boxplot(x=\"class\", y=key, data=train_clinical_df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647fe80-1722-4e87-a010-d1ef101febdc",
   "metadata": {},
   "source": [
    "## Building our model\n",
    "\n",
    "Here we will define a pipeline (`Pipeline`) containing a set of components which we have previously discussed are aluded to:\n",
    "\n",
    "- `VarianceThreshold` will (mostly) remove constant features\n",
    "- `SelectPercentile` will be selecting the top 10% of features which are the most impactful for prediction according to an ANOVA\n",
    "- `StandardScaler` will scale the data such that each feature has $\\mu=0$ and $\\sigma=1$\n",
    "- `RFECV` will perform recursive feature elimination (i.e. features are dropped whenever they are not sufficiently important for prediction)\n",
    "- `RandomForestClassifier` will be our random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2525209-20ff-47b1-a08a-5ce63920868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE, SelectKBest, VarianceThreshold, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [(\"variance_threshold\", VarianceThreshold()),\n",
    "     (\"scaler\", StandardScaler()),\n",
    "     # (\"ufs\", SelectKBest(k=100, score_func=f_classif)),\n",
    "     # (\"rfe\", RFE(estimator=RandomForestClassifier())),\n",
    "     (\"model\", RandomForestClassifier(\n",
    "         class_weight=\"balanced_subsample\", random_state=42))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f5377c-8551-45ab-887f-7fecaba752b9",
   "metadata": {},
   "source": [
    "To select the best features for `SGDClassifier`, we will wrap everything in a nested cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_radiomics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d482b-ee04-490a-b1b8-3f265805d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "curr_df = train_radiomics_df\n",
    "X = curr_df.drop(\"class\", axis=1).astype(np.float32).to_numpy()\n",
    "y = curr_df[\"class\"].to_numpy()\n",
    "\n",
    "params = {\"model__n_estimators\": randint(5, 100),\n",
    "          \"model__max_depth\": randint(5, 100),\n",
    "          \"model__ccp_alpha\": uniform(0.0, 0.5)}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "inner_model = RandomizedSearchCV(pipeline, params, cv=inner_cv, n_iter=100)\n",
    "nested_score = cross_validate(\n",
    "    inner_model, \n",
    "    X=X, \n",
    "    y=y, \n",
    "    cv=outer_cv, \n",
    "    scoring=\"roc_auc\", \n",
    "    verbose=4, \n",
    "    n_jobs=5)\n",
    "\n",
    "average_auc = np.mean(nested_score[\"test_score\"])\n",
    "print(average_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
